{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu_y9pRCbCkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fa165d-007e-4803-b907-b5c43a605e4d"
      },
      "source": [
        "!pip install bert4keras==0.7.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert4keras==0.7.5\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/18/a88400ab56b690afd23901f21f75714a60f84f6fe446641730708bbbeba5/bert4keras-0.7.5.tar.gz\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from bert4keras==0.7.5) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->bert4keras==0.7.5) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->bert4keras==0.7.5) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->bert4keras==0.7.5) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->bert4keras==0.7.5) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras->bert4keras==0.7.5) (1.15.0)\n",
            "Building wheels for collected packages: bert4keras\n",
            "  Building wheel for bert4keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert4keras: filename=bert4keras-0.7.5-cp37-none-any.whl size=36496 sha256=c6ac7776289169a8805bfadd7c2973d6b49208e388e059b643f90b189a9ca124\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/28/96/9400220273f85d1515b73017f74630d769d6f04523405e2de3\n",
            "Successfully built bert4keras\n",
            "Installing collected packages: bert4keras\n",
            "Successfully installed bert4keras-0.7.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjhWLWqY2xAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "630f6330-c59e-4e29-c2d7-acac77a8c43f"
      },
      "source": [
        "!pip install keras_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_bert\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_bert) (1.19.5)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from keras_bert) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras>=2.4.3->keras_bert) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras>=2.4.3->keras_bert) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras>=2.4.3->keras_bert) (1.4.1)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras>=2.4.3->keras_bert) (1.15.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp37-none-any.whl size=34144 sha256=23b4453ae50adc239df48db58565f2e4b661c1dfe0d5e825c9c640f95f2a1649\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp37-none-any.whl size=12942 sha256=b49211ad3339c76234c660a625c0028fd3205e434fb652ce5344f4ef50f942e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp37-none-any.whl size=7554 sha256=7add3f96e24cca2be85d7b70608108f5747e5d2f6c3cc30e3c51a4e6d57691da\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp37-none-any.whl size=15611 sha256=c4ac8df76e37153725bcd2bbff0f3a07c2b5e0ca2b9b2cd2df5131f67b70c2cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp37-none-any.whl size=5269 sha256=ea09e34871681a2758354a498ceefeef43c90d78aa19ae1edee8b3411eeb782d\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp37-none-any.whl size=5623 sha256=5cfab60f5fb66167af8205615ad8b08dee7547237fd7a0bfa49328f5113ab7a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp37-none-any.whl size=4558 sha256=33f4cbbf2c84b5bd54127618f156b988d052c27f7ff7a41fa5b581e5faa6427a\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp37-none-any.whl size=17278 sha256=84c9086603579940ceaa2e40c5f3438294688f524cde2d5455087a385781a6b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxD6FM-W3Qsu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffd0c4c8-12df-4317-c679-dafa6958552a"
      },
      "source": [
        "!pip install tensorflow==2.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/1a/0d79814736cfecc825ab8094b39648cc9c46af7af1bae839928acb73b4dd/tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 28kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (3.12.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.19.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.32.0)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 39.4MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.1.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.36.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.4.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow==2.2) (56.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.28.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.3.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.10.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.4.1)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOvRXivtW_6J"
      },
      "source": [
        "Plan1 succeed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ItFZn1qGjrV",
        "outputId": "30501b1c-fc95-48b0-908a-97d10457da59"
      },
      "source": [
        "from bert4keras.backend import keras\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.tokenizers import Tokenizer\n",
        "import numpy as np\n",
        "import json\n",
        "from keras.models import Model\n",
        "\n",
        "from bert4keras.backend import keras, K\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.tokenizers import Tokenizer\n",
        "from bert4keras.optimizers import Adam\n",
        "from bert4keras.snippets import sequence_padding, DataGenerator\n",
        "from bert4keras.snippets import open\n",
        "from bert4keras.layers import ConditionalRandomField\n",
        "from keras.layers import Dense\n",
        "from keras.models import Model\n",
        "from tqdm import tqdm\n",
        "from keras.layers import Dropout, Dense\n",
        "\n",
        "from keras_bert import extract_embeddings\n",
        "import tensorflow as tf\n",
        "\n",
        "config_path = '/content/bert_config.json'\n",
        "checkpoint_path = tf.train.latest_checkpoint('/content/')\n",
        "#checkpoint_path = '/content/bert_model.ckpt'\n",
        "dict_path = '/content/vocab.txt'\n",
        "\n",
        "def load_data(filename):\n",
        "\n",
        "    D = []\n",
        "    with open(filename,encoding='utf-8') as f:\n",
        "        for l in f:\n",
        "            l = json.loads(l)\n",
        "            D.append(l['text'])\n",
        "    return D\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    # 词向量获取方法 cls,mean,\n",
        "    vector_name = 'cls'\n",
        "    tokenizer = Tokenizer(dict_path, do_lower_case=True)  # 建立分词器\n",
        "    model = build_transformer_model(config_path, checkpoint_path)  # 建立模型，加载权重\n",
        "    maxlen = 70\n",
        "\n",
        "    # 读取处理数据\n",
        "    #f1 = 'D:/cluster/data/train.json'\n",
        "    #res = load_data(f1)\n",
        "    output = []\n",
        "    res = result\n",
        "\n",
        "    print('start to extract vector from BERT: ')\n",
        "\n",
        "    # 根据提取特征的方法获得词向量\n",
        "    for r in res:\n",
        "        token_ids, segment_ids = tokenizer.encode(r,max_length=maxlen)\n",
        "\n",
        "        if vector_name == 'cls':\n",
        "            cls_vector = model.predict([np.array([token_ids]), np.array([segment_ids])])[0][0]\n",
        "            output.append(cls_vector)\n",
        "        elif vector_name == 'mean':\n",
        "            new = []\n",
        "            vector = model.predict([np.array([token_ids]), np.array([segment_ids])])[0]\n",
        "            for i in range(768):\n",
        "                temp = 0\n",
        "                for j in range(len(vector)):\n",
        "                    temp += vector[j][i]\n",
        "                new.append(temp/(len(vector)))\n",
        "            output.append(new)\n",
        "\n",
        "    print('保存数据')\n",
        "    np.savetxt(\"/content/wendy_vector_cls.txt\",output)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start to extract vector from BERT: \n",
            "保存数据\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSputpMJHoPR",
        "outputId": "5ef21d5b-1c55-42d7-a57c-bddf8cc19782"
      },
      "source": [
        "count = 0\n",
        "with open('/content/test_data_annotation.txt',encoding='utf-8') as f:\n",
        "  for l in f:\n",
        "    count+=1\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYgF2Mtj1VKV"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "#显示所有行\n",
        "pd.set_option('display.max_rows', None)\n",
        "#设置value的显示长度为100，默认为50\n",
        "#pd.set_option('max_colwidth',500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGhf4G7O8Y-J",
        "outputId": "007c5c5f-54a2-41f3-8212-8ff92eb809da"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import Birch\n",
        "\n",
        "#feature = np.loadtxt(\"/content/bert_vector_cls.txt\")\n",
        "#feature = np.loadtxt(\"/content/bert_vector_mean.txt\")\n",
        "#feature = np.loadtxt(\"/content/bert_finetune_vector_cls.txt\")\n",
        "#feature = np.loadtxt(\"/content/bert_finetune_vector_mean.txt\")\n",
        "#feature = np.loadtxt(\"/content/bert_finetune260_vector_cls.txt\")\n",
        "#feature = np.loadtxt(\"/content/bert_finetune390_vector_cls.txt\")\n",
        "feature = np.loadtxt(\"/content/wendy_vector_cls.txt\")\n",
        "clf = KMeans(n_clusters=8)\n",
        "s = clf.fit(feature)\n",
        "kn_pre = clf.predict(feature)\n",
        "\n",
        "#cluster_assignment = s.labels_\n",
        "#print(cluster_assignment)\n",
        "\n",
        "ss = Birch(branching_factor=10, n_clusters = 8, threshold=0.5,compute_labels=True)\n",
        "birch_pre = ss.fit_predict(feature)\n",
        "print(kn_pre)\n",
        "#print(birch_pre)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4 6 1 7 5 0 2 1 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EGyoyl98nXI"
      },
      "source": [
        "!pip install hdbscan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erTveFX68fTO",
        "outputId": "d21af984-a2d0-423c-b2e1-9c51e5181cf0"
      },
      "source": [
        "import hdbscan\n",
        "cluster = hdbscan.HDBSCAN(min_cluster_size=8,metric='euclidean',cluster_selection_method='eom').fit(feature)\n",
        "cluster_assignment = cluster.labels_\n",
        "print(cluster_assignment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1  1  1 ...  1 -1  1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6lVuGL9Kmha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da51a881-f620-45a4-e8b2-28cd0b51bc28"
      },
      "source": [
        "print(kn_pre[:1083])\n",
        "with open(\"/content/bert_finetune390_cls_kmeans_test.txt\",'w') as wf:\n",
        "  wf.write(str(kn_pre[:1083].tolist()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 7 2 ... 6 4 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUA2_zAB4L6L",
        "outputId": "2fd616ac-3839-488a-fe99-5ca47fe3ca08"
      },
      "source": [
        "print(birch_pre[:1083])\n",
        "with open(\"/content/bert_finetune_mean_birch_test.txt\",'w') as wf:\n",
        "  wf.write(str(birch_pre[:1083].tolist()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 2 5 ... 1 0 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB_q26D-9033",
        "outputId": "363554ed-705f-410b-8226-22a6535d60df"
      },
      "source": [
        "print(cluster_assignment[:1083])\n",
        "with open(\"/content/bert_finetune_mean_hdbscan_test.txt\",'w') as wf:\n",
        "  wf.write(str(cluster_assignment[:1083].tolist()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1  1  1 ...  1 -1  1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVHC4Kxc4YrG"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAe9SKQj4YaX",
        "outputId": "7e472573-cf3b-49a6-e83e-441d5dd926c3"
      },
      "source": [
        "from sklearn import metrics\n",
        "#kmeans_model = KMeans(n_clusters=8, random_state=1).fit(X)\n",
        "labels = s.labels_\n",
        "score = metrics.calinski_harabaz_score(feature,labels)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "369.84697409965054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function calinski_harabaz_score is deprecated; Function 'calinski_harabaz_score' has been renamed to 'calinski_harabasz_score' and will be removed in version 0.23.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C9_gS6g6nA3",
        "outputId": "0f7b093a-282f-4037-d89b-bb5705dc23dc"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import datasets\n",
        "# 生成数据集\n",
        "\n",
        "# 轮廓系数\n",
        "metrics.silhouette_score(feature,labels, metric='euclidean')\n",
        "\n",
        "#> 0.5528190123564091"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03884904321108207"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgGknm9q7cgB",
        "outputId": "f9fc2c7e-d53b-4e25-b7dd-c0b97e6ce385"
      },
      "source": [
        "sklearn.metrics.davies_bouldin_score(feature,labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.5237417364175814"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSOKosxX9Db9"
      },
      "source": [
        "num_clusters = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irNGjFKm8_7i",
        "outputId": "ea86f176-bfe9-4ecb-d94d-ce149b596e8d"
      },
      "source": [
        "clustered_sentences = [[] for i in range(num_clusters)]\n",
        "for sentence_id, cluster_id in enumerate(kn_pre):\n",
        "    clustered_sentences[cluster_id].append(res[sentence_id])\n",
        "for i, cluster in enumerate(clustered_sentences):\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster  1\n",
            "['What might be surprising, for some readers, is to learn that Sarah Grimké is a historical figure, an energetic abolitionist from a slaveholding Charleston family. After moving to Philadelphia and becoming a Quaker, she began speaking publicly against slavery and crusading for women’s rights. With her sister, Angelina, and Angelina’s husband, Theodore Weld, she wrote “American Slavery as It Is,” a “testimony of a thousand witnesses” that influenced Harriet Beecher Stowe’s novel “Uncle Tom’s Cabin,” published 13 years later.']\n",
            "\n",
            "Cluster  2\n",
            "['To her credit, Kidd doesn’t insist on a close friendship between these characters. They like each other, but uneasily, Sarah out of guilt and Handful because she knows she’s listed on a household inventory “right after the water trough, the wheelbarrow, the claw hammer and the bushel of flint corn.” Instead, through alternating chapters narrated by Sarah and Handful, spanning 35 years, the novel juxtaposes their experiences of oppression. Plain but studious Sarah reads Voltaire, studies Latin with an older brother and dreams of becoming the first female jurist. But when she reveals this ambition to her father, a judge, he declares angrily that she’s speaking “nonsense.” Her mother later tells her that “every girl must have ambition knocked out of her for her own good” and forces her to start husband-hunting.', 'It’s curious, therefore, that of the two narrators, Handful is the more believable. She certainly has the more vivid voice. “The smells in there would knock you down,” she notes of the stalls at the Charleston market, and after a frightening moment reveals “my heart had been beat to butter.” By contrast, Sarah is given to weighty pronouncements: “For all my resistance about slavery, I breathed that foul air, too.” ']\n",
            "\n",
            "Cluster  3\n",
            "['Although notorious in their own time (“arguably the most famous,” according to Kidd, “as well as the most infamous women in America”), the Grimké sisters are less well known today. Kidd’s intention, as she explains in an author’s note, was to write not a “thinly fictionalized account” of Sarah Grimké’s life, but rather a “thickly imagined story” based on extensive biographical material, including diaries, letters and newspaper accounts. Handful, however, is almost wholly imagined. A slave named Hetty was indeed presented to Sarah Grimké, but the actual Hetty died in childhood and nothing more is known about her.']\n",
            "\n",
            "Cluster  4\n",
            "['Both Handful and Sarah are admirable characters, though rather disappointingly so. Improbable allies are most engaging when they make life hard for each other and generally it takes them a while to find their common pulse. But Sarah empathizes so completely with Handful from the very beginning that we never get to doubt their innate sisterhood. While their identities as mistress and slave imply conflict, it’s not a conflict played out between them. Handful’s rich resentment is rarely directed at Sarah. How could it be? The actual Sarah Grimké may have been as earnest and honorable as she is here, but a little less righteousness might have furnished this story with a wider wingspan. ']\n",
            "\n",
            "Cluster  5\n",
            "['Unlikely alliances are a staple of fiction, and the unlikelier the better, from Huck and Jim floating down the Mississippi to Frodo and Gollum creeping toward Mordor — because the real drama lies in watching how dissimilar characters turn out to be brothers (or sisters) under the skin. Sue Monk Kidd followed this principle in her best-selling first novel, “The Secret Life of Bees,” in which a 14-year-old white girl and her family’s black servant join in fleeing abuse in the South Carolina of the civil rights era. Kidd’s latest novel, “The Invention of Wings,” also set largely in South Carolina, involves another unusual duo, in this case a slave and a daughter of the family that owns her.']\n",
            "\n",
            "Cluster  6\n",
            "['Yet, as the novel’s title suggests, the desire for freedom inspires both heroines to defy their restrictions — one overtly, the other covertly. The first scene opens with Charlotte telling Handful that long ago “in Africa the people could fly.” She then pats the child’s shoulder blades, assuring her: “This all what left of your wings. They nothing but these flat bones now, but one day you gon get ’em back.” That Handful and Sarah both take flight by the end of the novel is not perhaps very surprising.']\n",
            "\n",
            "Cluster  7\n",
            "['The story begins in Charleston in 1803 on the day 11-year-old Sarah Grimké is given Hetty, or “Handful,” roughly her same age, as a birthday present. A born abolitionist whose earliest memory is of witnessing a slave being whipped (a trauma that’s responsible for the stammer that still afflicts her), Sarah immediately tries to “return” Handful. When this attempt fails, she writes an official “certificate of manumission,” which is promptly torn in two. Although Handful has to serve as Sarah’s personal maidservant, the girls share confidences and even an illicit picnic on the roof. Sarah also teaches Handful to read and write, an infraction that results in harsh penalties for both.']\n",
            "\n",
            "Cluster  8\n",
            "['Meanwhile, Handful’s mother, Charlotte, a rebellious and talented seamstress, makes a “story quilt” detailing the history of their bondage, beginning with the kidnapping of Handful’s “granny-mauma” in Africa. Early on, Charlotte is also hideously punished for stealing a bolt of cloth. Naturally, Handful distrusts all white people, even the painfully well-meaning Sarah, and soon turns rebellious herself. The truly harrowing moments in the book all belong to her and, inevitably, so does the larger share of the reader’s sympathy.']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcuakefr1qj-"
      },
      "source": [
        "!pip install bert4keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "564mtXfD3bjt"
      },
      "source": [
        "!pip install tensorflow==2.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sddHzcFbiqn",
        "outputId": "9d8c1fd3-f4b7-4427-d6af-3d254e0aa5ff"
      },
      "source": [
        "from bert4keras.backend import keras\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.tokenizers import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "config_path = '/content/bert_model/bert_config.json'\n",
        "checkpoint_path = '/content/bert_model/bert_model.ckpt'\n",
        "dict_path = '/content/bert_model/vocab.txt'\n",
        "\n",
        "tokenizer = Tokenizer(dict_path, do_lower_case=True)  # 建立分词器\n",
        "model = build_transformer_model(config_path, checkpoint_path)  # 建立模型，加载权重\n",
        "\n",
        "# 编码测试\n",
        "corpus = [\n",
        "             \"Vodafone Wins 20,000 Crore Tax Arbitration Case Against Government\",\n",
        "             \"Voda Idea shares jump nearly 15% as Vodafone wins retro tax case in Hague\",\n",
        "             \"Gold prices today fall for 4th time in 5 days, down ₹6500 from last month high\",\n",
        "             \"Silver futures slip 0.36% to Rs 59,415 per kg, down over 12% this week\",\n",
        "             \"Amazon unveils drone that films inside your home. What could go wrong?\",\n",
        "             \"IPHONE 12 MINI PERFORMANCE MAY DISAPPOINT DUE TO THE APPLE B14 CHIP\",\n",
        "             \"Delhi Capitals vs Chennai Super Kings: Prithvi Shaw shines as DC beat CSK to post second consecutive win in IPL\",\n",
        "             \"French Open 2020: Rafael Nadal handed tough draw in bid for record-equaling 20th Grand Slam\"\n",
        "]\n",
        "\n",
        "#print(corpus)\n",
        "token_ids, segment_ids = tokenizer.encode(corpus)\n",
        "\n",
        "\n",
        "print('\\n ===== predicting =====\\n')\n",
        "\n",
        "vector = model.predict([np.array([token_ids]), np.array([segment_ids])])\n",
        "print(vector.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ===== predicting =====\n",
            "\n",
            "(1, 8, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKPCX9FCugVW"
      },
      "source": [
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L91HYC2723Hw"
      },
      "source": [
        "Plan2 succeed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpt0cYVT24ZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79348ebb-53b3-4e24-a0d0-f318d0526ecf"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/87/49dc49e13ac107ce912c2f3f3fd92252c6d4221e88d1e6c16747044a11d8/sentence-transformers-1.1.0.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.1MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 15.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 50.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 33.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.1.0-cp37-none-any.whl size=119615 sha256=78afaf8b75ba4cd38f7202c8408a1b692aa8e3f523888657503d52fed24fa04d\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/cb/21/1066bff3027215c760ca14a198f698bca8fccb92e33e2327eb\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.45 sentence-transformers-1.1.0 sentencepiece-0.1.95 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw99-0a13enH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "294cac57-0feb-40fd-9a0e-ec99669d764f"
      },
      "source": [
        "corpus = [\n",
        "             \"Vodafone Wins 20,000 Crore Tax Arbitration Case Against Government\",\n",
        "             \"Voda Idea shares jump nearly 15% as Vodafone wins retro tax case in Hague\",\n",
        "             \"Gold prices today fall for 4th time in 5 days, down ₹6500 from last month high\",\n",
        "             \"Silver futures slip 0.36% to Rs 59,415 per kg, down over 12% this week\",\n",
        "             \"Amazon unveils drone that films inside your home. What could go wrong?\",\n",
        "             \"IPHONE 12 MINI PERFORMANCE MAY DISAPPOINT DUE TO THE APPLE B14 CHIP\",\n",
        "             \"Delhi Capitals vs Chennai Super Kings: Prithvi Shaw shines as DC beat CSK to post second consecutive win in IPL\",\n",
        "             \"French Open 2020: Rafael Nadal handed tough draw in bid for record-equaling 20th Grand Slam\"\n",
        "]\n",
        "\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Vodafone Wins 20,000 Crore Tax Arbitration Case Against Government', 'Voda Idea shares jump nearly 15% as Vodafone wins retro tax case in Hague', 'Gold prices today fall for 4th time in 5 days, down ₹6500 from last month high', 'Silver futures slip 0.36% to Rs 59,415 per kg, down over 12% this week', 'Amazon unveils drone that films inside your home. What could go wrong?', 'IPHONE 12 MINI PERFORMANCE MAY DISAPPOINT DUE TO THE APPLE B14 CHIP', 'Delhi Capitals vs Chennai Super Kings: Prithvi Shaw shines as DC beat CSK to post second consecutive win in IPL', 'French Open 2020: Rafael Nadal handed tough draw in bid for record-equaling 20th Grand Slam']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "xDQwz9iv3tA-",
        "outputId": "f69d76cb-82ea-48cd-9a1c-c688b245c349"
      },
      "source": [
        "path = \"/content/class1.txt\"\n",
        "#path = \"/content/test_data_annotation.txt\"\n",
        "import sys\n",
        "result=[]\n",
        "\n",
        "with open(path, 'r',encoding='utf8') as f:\n",
        "  for line in f:\n",
        "    line = line.replace(\"\\ufeff\",\"\")\n",
        "    result.append(line.strip('\\n'))\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ee03dc4b4dfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\ufeff\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa1 in position 266: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plj29RUY3CVk",
        "outputId": "c671263f-7956-4071-ff5a-ceb59bb58137"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "\n",
        "corpus_embeddings = embedder.encode(result)\n",
        "print(corpus_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.6162678  -0.03353756  0.19717501 ...  0.9555801  -0.5374956\n",
            "   0.3449843 ]\n",
            " [ 0.60900754  0.21543334  0.7866887  ...  0.54325205  0.02599801\n",
            "  -0.20927729]\n",
            " [-0.25242838 -0.1675365   0.8827442  ... -0.01020456 -0.03673603\n",
            "  -0.17408068]\n",
            " ...\n",
            " [ 0.07931372  0.7988257   0.55632484 ...  0.72487044 -0.2917974\n",
            "  -0.20245488]\n",
            " [-0.4437512  -0.00811411  0.44895118 ... -0.06891416 -0.14622977\n",
            "   0.29172873]\n",
            " [-0.0593288   0.6054962   0.71759695 ...  0.3600835  -0.7377779\n",
            "  -0.5549035 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwJRNRWsGypv"
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/snetence_bert_vector.txt\",corpus_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mPKEQIBF_qE",
        "outputId": "065651fc-b834-4100-aae3-0eb52d846561"
      },
      "source": [
        "from sklearn.cluster import Birch\n",
        "ss = Birch(branching_factor=10, n_clusters = 8, threshold=0.5,compute_labels=True)\n",
        "birch_pre = ss.fit_predict(corpus_embeddings)\n",
        "print(birch_pre)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 3 4 ... 3 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZm1CkUK-Ohk"
      },
      "source": [
        "!pip install hdbscan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_Y8LDsc-BXY",
        "outputId": "5d1558e9-50bf-4149-eb4b-37a6617dfae8"
      },
      "source": [
        "import hdbscan\n",
        "cluster = hdbscan.HDBSCAN(min_cluster_size=8,metric='euclidean',cluster_selection_method='eom').fit(corpus_embeddings)\n",
        "cluster_assignment = cluster.labels_\n",
        "print(cluster_assignment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfAbbGliAshA"
      },
      "source": [
        "!pip install umap-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ2K7t4AAxFG"
      },
      "source": [
        "import umap\n",
        "umap_embeddings = umap.UMAP(n_neighbors=8, n_components=5, metric='cosine').fit_transform(corpus_embeddings)\n",
        "#cluster_assignment = umap_embeddings.labels_\n",
        "print(umap_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zvooH2M4gea",
        "outputId": "ac6ba29d-86bf-4b7a-d0b8-f12ca10ce592"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "num_clusters = 8\n",
        "# Define kmeans model\n",
        "clustering_model = KMeans(n_clusters=num_clusters)\n",
        "# Fit the embedding with kmeans clustering.\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "# Get the cluster id assigned to each news headline.\n",
        "kn_pre = clustering_model.labels_\n",
        "print(kn_pre)\n",
        "kn_centroid = clustering_model.cluster_centers_\n",
        "print(kn_centroid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 1 4 3 2 2]\n",
            "[[ 0.61263764  0.09094789  0.49193186 ...  0.74941605 -0.2557488\n",
            "   0.0678535 ]\n",
            " [ 0.26470578 -0.3152468   0.68221474 ... -0.3334794   0.2840914\n",
            "   0.05381416]\n",
            " [-0.25154     0.29869106  0.58327407 ...  0.14558467 -0.44200385\n",
            "  -0.1315874 ]\n",
            " [ 0.07931371  0.7988257   0.55632484 ...  0.72487044 -0.2917974\n",
            "  -0.20245488]\n",
            " [ 0.5291232   0.2117285   0.02326873 ... -0.4247378   0.33376944\n",
            "   0.19550073]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nN_2DWKIIHM",
        "outputId": "7197d3da-14a9-4b03-b507-bdf3b07d9d09"
      },
      "source": [
        "print(kn_pre[:1083])\n",
        "with open(\"/content/sentence_bert_kmeans_test.txt\",'w') as wf:\n",
        "  wf.write(str(kn_pre[:1083].tolist()))\n",
        "print(birch_pre[:1083])\n",
        "with open(\"/content/sentence_bert_birch_test.txt\",'w') as wf:\n",
        "  wf.write(str(birch_pre[:1083].tolist()))\n",
        "print(cluster_assignment[:1083])\n",
        "with open(\"/content/sentence_bert_hdbscan_test.txt\",'w') as wf:\n",
        "  wf.write(str(cluster_assignment[:1083].tolist()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7 3 1 ... 3 0 3]\n",
            "[1 3 4 ... 4 0 1]\n",
            "[ 0  0  0 ...  0 -1  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKj9jxFAoziL"
      },
      "source": [
        "with open(\"/content/bert_finetune260_cls_kmeans_test.txt\",'w') as wf:\n",
        "  wf.write(str(kn_pre[:1083].tolist()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqeD6TPUJVFh",
        "outputId": "9ff33c40-7c3a-4505-91c5-e12b2a0834d8"
      },
      "source": [
        "from sklearn import metrics\n",
        "#metrics.calinski_harabaz_score(corpus_embeddings,cluster_assignment)\n",
        "#metrics.silhouette_score(corpus_embeddings,cluster_assignment, metric='euclidean')\n",
        "sklearn.metrics.davies_bouldin_score(corpus_embeddings,cluster_assignment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.479092815962237"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtEOddwL4tBH"
      },
      "source": [
        "clustered_sentences = [[] for i in range(num_clusters)]\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "for i, cluster in enumerate(clustered_sentences):\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nEDNAHj7MbX"
      },
      "source": [
        "annotation的数据来了"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp0FZ6bc7Pck",
        "outputId": "5974875a-36ae-4954-ecce-9058bdd0762b"
      },
      "source": [
        "count0 = list(kn_pre[:1083]).count(0)\n",
        "count1 = list(kn_pre[:1083]).count(1)\n",
        "count2 = list(kn_pre[:1083]).count(2)\n",
        "count3 = list(kn_pre[:1083]).count(3)\n",
        "count4 = list(kn_pre[:1083]).count(4)\n",
        "count5 = list(kn_pre[:1083]).count(5)\n",
        "count6 = list(kn_pre[:1083]).count(6)\n",
        "count7 = list(kn_pre[:1083]).count(7)\n",
        "print(count0,count1,count2,count3,count4,count5,count6,count7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZWRJT2wOQBm",
        "outputId": "e00d424b-099c-4e64-b16f-e47578431904"
      },
      "source": [
        "count0 = list(true_labels).count(0)\n",
        "count1 = list(true_labels).count(1)\n",
        "count2 = list(kn_pre[:1083]).count(2)\n",
        "count3 = list(kn_pre[:1083]).count(3)\n",
        "count4 = list(kn_pre[:1083]).count(4)\n",
        "count5 = list(kn_pre[:1083]).count(5)\n",
        "count6 = list(kn_pre[:1083]).count(6)\n",
        "count7 = list(kn_pre[:1083]).count(7)\n",
        "print(count0,count1,count2,count3,count4,count5,count6,count7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 130 158 158 156 65 204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcOkD-sHA20W"
      },
      "source": [
        "Purity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V7Mv-OFA6KA",
        "outputId": "8da0c3b7-1b81-4c8a-d865-c374e91f10e1"
      },
      "source": [
        "with open(\"/content/annotation_labels.txt\",'r') as f:\n",
        "  for line in f:\n",
        "    true_labels = line\n",
        "print(true_labels)\n",
        "list_labels = true_labels.split(\" \")\n",
        "print(list_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 2 3 7 5 5 3 5 6 6 6 6 6 6 6 6 6 6 6 6 3 3 6 6 6 6 3 3 6 6 6 6 6 6 2 2 6 6 6 3 3 2 5 6 6 6 6 7 7 7 7 1 1 1 2 6 6 6 3 6 7 7 7 3 3 4 3 6 6 6 6 6 3 6 6 3 3 3 3 3 7 4 7 7 7 7 8 8 8 8 8 7 7 1 1 1 3 3 3 3 2 2 2 3 2 2 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 2 2 2 2 6 6 6 2 3 6 6 6 6 6 7 3 6 6 6 6 6 6 6 6 6 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 6 6 6 6 6 6 6 6 8 8 1 1 1 3 3 3 2 6 6 6 3 3 3 6 6 6 6 6 6 5 6 6 6 3 3 3 3 6 6 6 6 5 5 5 6 6 5 3 3 7 7 7 6 6 6 6 6 6 6 6 6 3 7 7 7 7 3 3 7 7 8 8 1 1 1 1 1 2 2 2 2 2 3 5 6 6 5 5 6 6 6 6 6 6 6 6 6 3 7 7 7 7 7 7 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 6 6 6 7 7 8 1 1 1 2 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 8 8 1 1 1 2 4 4 4 4 4 4 4 7 2 5 6 6 6 6 6 6 8 8 8 8 8 8 8 7 5 7 7 5 5 5 6 6 6 6 6 7 7 7 8 5 5 5 5 7 7 7 7 8 3 7 7 7 7 6 6 6 8 8 8 8 7 8 8 8 7 1 1 1 7 7 4 8 6 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 2 2 2 2 2 5 5 5 5 7 6 5 5 5 5 6 6 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 2 2 7 7 8 7 7 7 3 3 3 3 3 3 3 3 8 8 8 8 8 8 3 3 7 7 5 4 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 3 6 6 6 6 6 6 7 6 2 6 5 6 6 7 5 5 5 5 5 5 6 6 2 6 6 6 6 6 6 6 7 3 5 7 6 6 6 6 6 5 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 6 6 6 6 6 6 3 3 6 6 6 6 6 6 7 6 6 3 7 5 5 5 5 2 8 7 5 6 6 6 6 6 6 7 7 7 7 7 7 7 7 3 3 8 8 8 7 7 7 7 3 3 7 7 6 7 7 7 7 6 2 2 2 2 2 2 2 8 7 3 7 7 7 6 7 7 2 8 7 7 8 8 7 7 7 7 8 7 7 7 2 2 4 4 4 7 2 8 5 6 6 6 5 6 6 6 6 5 5 5 5 5 6 6 7 7 6 6 6 6 6 6 6 1 1 1 5 5 5 6 6 7 5 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 3 3 3 5 5 6 6 6 6 6 7 6 5 5 5 6 6 7 7 7 6 7 7 7 3 3 6 6 6 6 6 6 6 7 8 8 1 1 1 1 3 7 7 7 8 3 6 6 6 7 7 7 7 6 6 6 6 7 7 6 6 6 6 6 6 6 6 6 7 3 3 6 6 6 3 7 7 3 6 6 7 7 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 8 7 3 8 1 1 7 7 7 7 7 7 7 7 3 3 2 3 3 3 8 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 8 8 7 7 6 6 6 2 7 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 1 1 1 2 6 6 6 6 6 4 4 4 4 4 4 4 6 6 6 3 3 3 5 3 3 3 5 5 5 5 5 5 5 3 7 2 2 4 4 2 2 5 5 6 3 3 3 7 8 7 7 7 7 7 2 2 2 3 3 6 6 6 7 7 7 6 6 6 6 6 6 6 6 5 6 6 6 6 6 6 6 6 6 5 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 6 6 6 6 6 6 6 6 6 6 6 6 8 8 8 8 8\n",
            "['2', '2', '3', '7', '5', '5', '3', '5', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '3', '3', '6', '6', '6', '6', '3', '3', '6', '6', '6', '6', '6', '6', '2', '2', '6', '6', '6', '3', '3', '2', '5', '6', '6', '6', '6', '7', '7', '7', '7', '1', '1', '1', '2', '6', '6', '6', '3', '6', '7', '7', '7', '3', '3', '4', '3', '6', '6', '6', '6', '6', '3', '6', '6', '3', '3', '3', '3', '3', '7', '4', '7', '7', '7', '7', '8', '8', '8', '8', '8', '7', '7', '1', '1', '1', '3', '3', '3', '3', '2', '2', '2', '3', '2', '2', '2', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '2', '2', '2', '2', '6', '6', '6', '2', '3', '6', '6', '6', '6', '6', '7', '3', '6', '6', '6', '6', '6', '6', '6', '6', '6', '3', '3', '3', '3', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '3', '6', '6', '6', '6', '6', '6', '6', '6', '8', '8', '1', '1', '1', '3', '3', '3', '2', '6', '6', '6', '3', '3', '3', '6', '6', '6', '6', '6', '6', '5', '6', '6', '6', '3', '3', '3', '3', '6', '6', '6', '6', '5', '5', '5', '6', '6', '5', '3', '3', '7', '7', '7', '6', '6', '6', '6', '6', '6', '6', '6', '6', '3', '7', '7', '7', '7', '3', '3', '7', '7', '8', '8', '1', '1', '1', '1', '1', '2', '2', '2', '2', '2', '3', '5', '6', '6', '5', '5', '6', '6', '6', '6', '6', '6', '6', '6', '6', '3', '7', '7', '7', '7', '7', '7', '5', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '3', '6', '6', '6', '7', '7', '8', '1', '1', '1', '2', '5', '5', '5', '5', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '7', '7', '7', '7', '7', '8', '8', '1', '1', '1', '2', '4', '4', '4', '4', '4', '4', '4', '7', '2', '5', '6', '6', '6', '6', '6', '6', '8', '8', '8', '8', '8', '8', '8', '7', '5', '7', '7', '5', '5', '5', '6', '6', '6', '6', '6', '7', '7', '7', '8', '5', '5', '5', '5', '7', '7', '7', '7', '8', '3', '7', '7', '7', '7', '6', '6', '6', '8', '8', '8', '8', '7', '8', '8', '8', '7', '1', '1', '1', '7', '7', '4', '8', '6', '5', '5', '5', '5', '5', '5', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '7', '3', '3', '3', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '2', '2', '2', '2', '2', '5', '5', '5', '5', '7', '6', '5', '5', '5', '5', '6', '6', '5', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '2', '2', '7', '7', '8', '7', '7', '7', '3', '3', '3', '3', '3', '3', '3', '3', '8', '8', '8', '8', '8', '8', '3', '3', '7', '7', '5', '4', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '3', '3', '6', '6', '6', '6', '6', '6', '7', '6', '2', '6', '5', '6', '6', '7', '5', '5', '5', '5', '5', '5', '6', '6', '2', '6', '6', '6', '6', '6', '6', '6', '7', '3', '5', '7', '6', '6', '6', '6', '6', '5', '6', '6', '6', '6', '6', '6', '6', '6', '5', '5', '5', '5', '5', '5', '5', '6', '6', '6', '6', '6', '6', '3', '3', '6', '6', '6', '6', '6', '6', '7', '6', '6', '3', '7', '5', '5', '5', '5', '2', '8', '7', '5', '6', '6', '6', '6', '6', '6', '7', '7', '7', '7', '7', '7', '7', '7', '3', '3', '8', '8', '8', '7', '7', '7', '7', '3', '3', '7', '7', '6', '7', '7', '7', '7', '6', '2', '2', '2', '2', '2', '2', '2', '8', '7', '3', '7', '7', '7', '6', '7', '7', '2', '8', '7', '7', '8', '8', '7', '7', '7', '7', '8', '7', '7', '7', '2', '2', '4', '4', '4', '7', '2', '8', '5', '6', '6', '6', '5', '6', '6', '6', '6', '5', '5', '5', '5', '5', '6', '6', '7', '7', '6', '6', '6', '6', '6', '6', '6', '1', '1', '1', '5', '5', '5', '6', '6', '7', '5', '3', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '3', '3', '3', '3', '5', '5', '6', '6', '6', '6', '6', '7', '6', '5', '5', '5', '6', '6', '7', '7', '7', '6', '7', '7', '7', '3', '3', '6', '6', '6', '6', '6', '6', '6', '7', '8', '8', '1', '1', '1', '1', '3', '7', '7', '7', '8', '3', '6', '6', '6', '7', '7', '7', '7', '6', '6', '6', '6', '7', '7', '6', '6', '6', '6', '6', '6', '6', '6', '6', '7', '3', '3', '6', '6', '6', '3', '7', '7', '3', '6', '6', '7', '7', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '8', '7', '3', '8', '1', '1', '7', '7', '7', '7', '7', '7', '7', '7', '3', '3', '2', '3', '3', '3', '8', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '8', '8', '7', '7', '6', '6', '6', '2', '7', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '1', '1', '1', '2', '6', '6', '6', '6', '6', '4', '4', '4', '4', '4', '4', '4', '6', '6', '6', '3', '3', '3', '5', '3', '3', '3', '5', '5', '5', '5', '5', '5', '5', '3', '7', '2', '2', '4', '4', '2', '2', '5', '5', '6', '3', '3', '3', '7', '8', '7', '7', '7', '7', '7', '2', '2', '2', '3', '3', '6', '6', '6', '7', '7', '7', '6', '6', '6', '6', '6', '6', '6', '6', '5', '6', '6', '6', '6', '6', '6', '6', '6', '6', '5', '3', '3', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '7', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '8', '8', '8', '8', '8']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqBFpEy9L0oh",
        "outputId": "2de1d7cf-98ec-43c1-fec7-04b472ca31dc"
      },
      "source": [
        "with open(\"/content/bert_finetune_mean_birch_test.txt\",'r') as f:\n",
        "  for line in f:\n",
        "    line = line.replace(\",\",\"\")\n",
        "    line = line.replace(\"[\",\"\")\n",
        "    true_labels = line.replace(\"]\",\"\")\n",
        "print(true_labels)\n",
        "y_pred = true_labels.split(\" \")\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 2 5 3 6 1 2 1 1 1 0 1 2 7 2 1 7 2 4 2 3 5 1 3 3 2 2 6 5 7 2 3 3 6 3 2 2 3 2 1 6 1 2 3 2 5 5 2 3 0 7 6 7 0 5 5 0 5 1 1 1 5 3 5 5 3 1 5 5 2 3 2 1 5 5 1 3 5 3 1 5 5 2 2 5 2 1 1 1 1 1 1 1 6 0 4 3 3 5 1 5 5 7 2 1 2 5 5 1 1 7 7 5 7 0 0 2 1 4 7 6 5 3 3 3 5 3 3 2 1 6 2 5 6 3 1 3 5 1 6 5 6 3 3 6 1 3 5 2 5 5 1 2 3 3 6 2 5 5 0 0 7 1 3 2 1 1 2 6 0 5 2 3 6 5 2 6 2 3 7 3 2 3 6 3 5 2 7 1 1 3 2 7 6 2 3 3 7 3 3 5 1 7 2 5 1 7 1 5 5 3 6 2 2 2 3 2 1 5 6 0 4 3 5 1 3 3 1 1 5 5 1 7 2 5 2 3 7 1 3 2 1 3 2 3 5 1 3 2 2 3 1 2 3 2 5 1 3 1 1 5 3 1 1 3 5 3 3 1 3 2 1 1 1 1 5 5 2 3 1 2 3 6 2 0 4 5 1 5 1 5 5 5 1 3 1 5 1 3 2 5 1 1 5 6 2 1 3 7 6 2 3 1 2 5 1 3 3 1 1 3 6 1 5 5 5 5 3 5 5 1 7 3 1 3 2 5 6 4 4 5 2 3 5 3 1 3 1 3 6 1 1 3 1 5 1 2 3 6 3 6 3 6 5 1 3 1 0 6 0 0 1 3 3 5 3 3 3 6 2 1 5 1 1 2 2 2 1 3 6 7 1 3 1 3 2 3 5 1 3 3 3 1 2 1 7 0 6 7 6 3 1 7 3 2 1 5 1 3 2 1 1 3 2 3 3 3 7 1 1 3 3 2 2 1 3 1 7 2 4 1 0 3 1 6 7 2 2 2 3 6 1 5 2 7 2 1 5 1 3 7 2 3 3 0 2 5 1 3 1 6 3 2 6 5 7 1 1 7 2 5 2 0 5 0 3 6 3 7 7 5 6 3 3 1 3 5 2 3 6 1 2 7 3 2 2 3 5 3 5 2 5 5 7 1 1 0 2 5 4 1 2 5 2 1 1 3 1 7 1 1 3 3 6 5 1 3 3 7 3 3 7 5 1 2 3 2 5 5 5 5 1 5 0 7 7 5 3 5 3 7 5 1 2 1 5 1 1 1 1 1 1 4 4 3 2 3 5 1 3 1 6 2 2 1 5 3 3 2 1 3 2 1 1 2 1 3 3 3 1 5 1 2 1 3 3 1 2 1 3 3 3 7 1 6 5 2 3 1 2 5 1 3 3 2 2 3 3 1 3 1 3 5 2 7 2 3 2 2 2 7 3 1 3 3 1 1 2 1 2 3 7 1 3 3 7 1 1 1 7 2 3 1 2 1 2 2 3 7 2 3 0 6 7 3 3 3 5 3 2 2 6 1 7 2 3 1 1 0 3 5 3 1 1 1 5 5 2 1 3 1 7 1 0 0 5 2 2 5 3 3 3 2 5 5 1 3 5 3 1 1 1 3 5 1 1 7 1 5 2 6 1 1 7 6 7 2 3 0 5 5 5 5 5 7 3 3 3 1 3 1 3 2 2 5 1 3 3 2 2 1 2 6 2 6 0 4 1 2 1 5 3 2 3 5 1 6 5 0 3 3 3 1 5 4 7 1 1 3 1 1 3 1 1 3 1 7 5 2 6 6 6 1 7 5 3 3 5 1 3 3 5 1 0 2 3 1 3 4 3 3 0 1 0 1 1 6 3 6 4 0 4 5 5 7 3 6 1 5 7 6 2 1 1 2 5 2 0 3 5 1 3 3 1 7 3 2 1 3 5 3 5 6 1 5 5 3 3 2 3 3 2 5 7 2 2 1 6 0 6 5 1 5 7 2 2 7 6 0 1 7 1 5 5 3 6 2 2 1 7 1 1 1 3 5 5 5 1 1 2 3 5 5 1 7 5 5 2 2 1 5 1 7 1 5 7 3 7 2 2 7 1 3 5 1 3 1 2 3 2 3 6 1 1 2 7 6 3 2 3 2 3 3 1 7 1 1 1 6 0 7 3 0 7 3 6 4 4 5 2 4 3 6 3 1 6 0 5 1 5 7 5 1 7 2 3 2 1 3 1 5 3 6 3 1 3 2 3 3 2 5 2 5 5 1 1 1 5 3 5 2 1 5 1 1 5 5 3 5 5 5 7 5 6 5 1 1 1 2 5 2 1 3 7 0 0 2 5 3 3 5 2 3 5 3 2 6 2 2 5 1 2 7 7 3 2 7 1 3 2 0 0 3 2 7 3 7 3 1 6 6 3 3 7 5 5 2 5 6 1 1 0 2\n",
            "['3', '2', '5', '3', '6', '1', '2', '1', '1', '1', '0', '1', '2', '7', '2', '1', '7', '2', '4', '2', '3', '5', '1', '3', '3', '2', '2', '6', '5', '7', '2', '3', '3', '6', '3', '2', '2', '3', '2', '1', '6', '1', '2', '3', '2', '5', '5', '2', '3', '0', '7', '6', '7', '0', '5', '5', '0', '5', '1', '1', '1', '5', '3', '5', '5', '3', '1', '5', '5', '2', '3', '2', '1', '5', '5', '1', '3', '5', '3', '1', '5', '5', '2', '2', '5', '2', '1', '1', '1', '1', '1', '1', '1', '6', '0', '4', '3', '3', '5', '1', '5', '5', '7', '2', '1', '2', '5', '5', '1', '1', '7', '7', '5', '7', '0', '0', '2', '1', '4', '7', '6', '5', '3', '3', '3', '5', '3', '3', '2', '1', '6', '2', '5', '6', '3', '1', '3', '5', '1', '6', '5', '6', '3', '3', '6', '1', '3', '5', '2', '5', '5', '1', '2', '3', '3', '6', '2', '5', '5', '0', '0', '7', '1', '3', '2', '1', '1', '2', '6', '0', '5', '2', '3', '6', '5', '2', '6', '2', '3', '7', '3', '2', '3', '6', '3', '5', '2', '7', '1', '1', '3', '2', '7', '6', '2', '3', '3', '7', '3', '3', '5', '1', '7', '2', '5', '1', '7', '1', '5', '5', '3', '6', '2', '2', '2', '3', '2', '1', '5', '6', '0', '4', '3', '5', '1', '3', '3', '1', '1', '5', '5', '1', '7', '2', '5', '2', '3', '7', '1', '3', '2', '1', '3', '2', '3', '5', '1', '3', '2', '2', '3', '1', '2', '3', '2', '5', '1', '3', '1', '1', '5', '3', '1', '1', '3', '5', '3', '3', '1', '3', '2', '1', '1', '1', '1', '5', '5', '2', '3', '1', '2', '3', '6', '2', '0', '4', '5', '1', '5', '1', '5', '5', '5', '1', '3', '1', '5', '1', '3', '2', '5', '1', '1', '5', '6', '2', '1', '3', '7', '6', '2', '3', '1', '2', '5', '1', '3', '3', '1', '1', '3', '6', '1', '5', '5', '5', '5', '3', '5', '5', '1', '7', '3', '1', '3', '2', '5', '6', '4', '4', '5', '2', '3', '5', '3', '1', '3', '1', '3', '6', '1', '1', '3', '1', '5', '1', '2', '3', '6', '3', '6', '3', '6', '5', '1', '3', '1', '0', '6', '0', '0', '1', '3', '3', '5', '3', '3', '3', '6', '2', '1', '5', '1', '1', '2', '2', '2', '1', '3', '6', '7', '1', '3', '1', '3', '2', '3', '5', '1', '3', '3', '3', '1', '2', '1', '7', '0', '6', '7', '6', '3', '1', '7', '3', '2', '1', '5', '1', '3', '2', '1', '1', '3', '2', '3', '3', '3', '7', '1', '1', '3', '3', '2', '2', '1', '3', '1', '7', '2', '4', '1', '0', '3', '1', '6', '7', '2', '2', '2', '3', '6', '1', '5', '2', '7', '2', '1', '5', '1', '3', '7', '2', '3', '3', '0', '2', '5', '1', '3', '1', '6', '3', '2', '6', '5', '7', '1', '1', '7', '2', '5', '2', '0', '5', '0', '3', '6', '3', '7', '7', '5', '6', '3', '3', '1', '3', '5', '2', '3', '6', '1', '2', '7', '3', '2', '2', '3', '5', '3', '5', '2', '5', '5', '7', '1', '1', '0', '2', '5', '4', '1', '2', '5', '2', '1', '1', '3', '1', '7', '1', '1', '3', '3', '6', '5', '1', '3', '3', '7', '3', '3', '7', '5', '1', '2', '3', '2', '5', '5', '5', '5', '1', '5', '0', '7', '7', '5', '3', '5', '3', '7', '5', '1', '2', '1', '5', '1', '1', '1', '1', '1', '1', '4', '4', '3', '2', '3', '5', '1', '3', '1', '6', '2', '2', '1', '5', '3', '3', '2', '1', '3', '2', '1', '1', '2', '1', '3', '3', '3', '1', '5', '1', '2', '1', '3', '3', '1', '2', '1', '3', '3', '3', '7', '1', '6', '5', '2', '3', '1', '2', '5', '1', '3', '3', '2', '2', '3', '3', '1', '3', '1', '3', '5', '2', '7', '2', '3', '2', '2', '2', '7', '3', '1', '3', '3', '1', '1', '2', '1', '2', '3', '7', '1', '3', '3', '7', '1', '1', '1', '7', '2', '3', '1', '2', '1', '2', '2', '3', '7', '2', '3', '0', '6', '7', '3', '3', '3', '5', '3', '2', '2', '6', '1', '7', '2', '3', '1', '1', '0', '3', '5', '3', '1', '1', '1', '5', '5', '2', '1', '3', '1', '7', '1', '0', '0', '5', '2', '2', '5', '3', '3', '3', '2', '5', '5', '1', '3', '5', '3', '1', '1', '1', '3', '5', '1', '1', '7', '1', '5', '2', '6', '1', '1', '7', '6', '7', '2', '3', '0', '5', '5', '5', '5', '5', '7', '3', '3', '3', '1', '3', '1', '3', '2', '2', '5', '1', '3', '3', '2', '2', '1', '2', '6', '2', '6', '0', '4', '1', '2', '1', '5', '3', '2', '3', '5', '1', '6', '5', '0', '3', '3', '3', '1', '5', '4', '7', '1', '1', '3', '1', '1', '3', '1', '1', '3', '1', '7', '5', '2', '6', '6', '6', '1', '7', '5', '3', '3', '5', '1', '3', '3', '5', '1', '0', '2', '3', '1', '3', '4', '3', '3', '0', '1', '0', '1', '1', '6', '3', '6', '4', '0', '4', '5', '5', '7', '3', '6', '1', '5', '7', '6', '2', '1', '1', '2', '5', '2', '0', '3', '5', '1', '3', '3', '1', '7', '3', '2', '1', '3', '5', '3', '5', '6', '1', '5', '5', '3', '3', '2', '3', '3', '2', '5', '7', '2', '2', '1', '6', '0', '6', '5', '1', '5', '7', '2', '2', '7', '6', '0', '1', '7', '1', '5', '5', '3', '6', '2', '2', '1', '7', '1', '1', '1', '3', '5', '5', '5', '1', '1', '2', '3', '5', '5', '1', '7', '5', '5', '2', '2', '1', '5', '1', '7', '1', '5', '7', '3', '7', '2', '2', '7', '1', '3', '5', '1', '3', '1', '2', '3', '2', '3', '6', '1', '1', '2', '7', '6', '3', '2', '3', '2', '3', '3', '1', '7', '1', '1', '1', '6', '0', '7', '3', '0', '7', '3', '6', '4', '4', '5', '2', '4', '3', '6', '3', '1', '6', '0', '5', '1', '5', '7', '5', '1', '7', '2', '3', '2', '1', '3', '1', '5', '3', '6', '3', '1', '3', '2', '3', '3', '2', '5', '2', '5', '5', '1', '1', '1', '5', '3', '5', '2', '1', '5', '1', '1', '5', '5', '3', '5', '5', '5', '7', '5', '6', '5', '1', '1', '1', '2', '5', '2', '1', '3', '7', '0', '0', '2', '5', '3', '3', '5', '2', '3', '5', '3', '2', '6', '2', '2', '5', '1', '2', '7', '7', '3', '2', '7', '1', '3', '2', '0', '0', '3', '2', '7', '3', '7', '3', '1', '6', '6', '3', '3', '7', '5', '5', '2', '5', '6', '1', '1', '0', '2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8hpL8ZaCDey",
        "outputId": "87ced2ef-7aa1-4bca-9ef0-7e4a04ac1675"
      },
      "source": [
        "y_pred = kn_pre[:1083]\n",
        "print(y_pred.tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7, 3, 1, 3, 1, 7, 7, 2, 4, 1, 0, 5, 0, 6, 1, 4, 2, 5, 0, 1, 7, 7, 4, 4, 5, 2, 7, 6, 4, 2, 2, 7, 7, 2, 2, 3, 7, 3, 0, 4, 2, 1, 2, 4, 3, 4, 4, 2, 7, 6, 6, 0, 0, 0, 5, 5, 6, 4, 7, 4, 7, 5, 3, 1, 4, 5, 7, 5, 7, 3, 4, 3, 2, 7, 7, 7, 5, 7, 7, 5, 7, 7, 0, 6, 7, 5, 3, 3, 7, 5, 7, 2, 5, 0, 0, 0, 7, 3, 4, 4, 3, 3, 0, 2, 4, 6, 2, 7, 4, 4, 2, 4, 4, 4, 2, 6, 1, 3, 6, 6, 2, 4, 0, 1, 1, 4, 1, 5, 7, 4, 3, 5, 3, 0, 4, 4, 0, 4, 3, 4, 3, 3, 7, 5, 0, 4, 4, 3, 2, 5, 3, 5, 3, 3, 1, 6, 3, 4, 7, 0, 0, 0, 4, 5, 3, 5, 5, 5, 6, 0, 5, 5, 5, 3, 4, 3, 3, 1, 1, 3, 5, 1, 5, 4, 2, 4, 5, 0, 3, 4, 7, 1, 2, 7, 2, 5, 5, 6, 6, 1, 3, 4, 5, 5, 5, 7, 1, 3, 7, 4, 7, 5, 7, 3, 5, 3, 5, 2, 4, 0, 0, 0, 1, 4, 1, 1, 4, 4, 3, 7, 4, 1, 1, 3, 4, 6, 6, 6, 2, 7, 3, 3, 2, 2, 7, 7, 7, 7, 6, 7, 7, 7, 2, 6, 2, 7, 6, 4, 7, 6, 1, 2, 4, 3, 3, 3, 4, 4, 4, 7, 3, 7, 7, 3, 2, 7, 2, 7, 7, 7, 2, 1, 0, 0, 0, 0, 1, 4, 1, 4, 3, 5, 5, 1, 3, 5, 4, 5, 3, 3, 7, 5, 5, 3, 2, 6, 5, 7, 0, 2, 7, 7, 3, 2, 4, 4, 5, 7, 3, 5, 3, 3, 1, 7, 3, 4, 4, 4, 3, 7, 4, 3, 3, 7, 3, 2, 4, 1, 0, 0, 7, 1, 5, 5, 3, 4, 5, 5, 3, 5, 5, 5, 4, 3, 5, 2, 5, 5, 5, 5, 7, 5, 7, 3, 3, 7, 7, 0, 2, 0, 0, 5, 2, 5, 5, 7, 3, 2, 2, 7, 7, 5, 3, 5, 3, 3, 5, 5, 2, 2, 2, 2, 2, 7, 2, 3, 5, 3, 7, 1, 5, 5, 3, 2, 5, 2, 0, 3, 6, 2, 7, 5, 0, 5, 5, 7, 1, 1, 3, 2, 7, 7, 2, 0, 7, 3, 3, 0, 3, 7, 3, 7, 5, 7, 2, 7, 7, 0, 0, 0, 7, 0, 2, 1, 6, 0, 1, 2, 3, 1, 2, 4, 1, 1, 1, 1, 1, 4, 1, 4, 3, 7, 1, 5, 0, 3, 4, 4, 4, 1, 3, 4, 7, 2, 4, 2, 4, 3, 7, 1, 4, 7, 0, 4, 0, 5, 1, 4, 3, 0, 7, 5, 7, 1, 4, 4, 4, 2, 5, 2, 5, 2, 2, 4, 1, 1, 2, 3, 7, 4, 2, 1, 5, 2, 4, 7, 0, 2, 5, 0, 7, 7, 4, 5, 3, 3, 3, 5, 3, 7, 4, 1, 7, 6, 7, 5, 7, 3, 6, 7, 7, 0, 7, 7, 6, 7, 6, 7, 1, 4, 7, 1, 7, 0, 0, 2, 7, 1, 7, 7, 6, 7, 5, 5, 1, 7, 7, 5, 5, 7, 1, 4, 6, 0, 5, 5, 1, 7, 7, 7, 3, 7, 5, 5, 7, 4, 5, 7, 7, 7, 1, 4, 7, 3, 7, 5, 5, 5, 1, 7, 4, 7, 2, 4, 7, 5, 2, 5, 1, 2, 7, 4, 6, 2, 2, 4, 6, 1, 4, 7, 4, 5, 5, 5, 2, 5, 5, 2, 4, 5, 4, 2, 3, 5, 0, 7, 4, 2, 0, 1, 2, 5, 4, 2, 3, 4, 3, 3, 5, 3, 4, 6, 5, 5, 2, 0, 4, 5, 3, 2, 6, 7, 7, 5, 1, 3, 2, 5, 5, 5, 5, 6, 0, 0, 6, 7, 3, 4, 6, 2, 2, 6, 2, 1, 2, 7, 7, 1, 0, 2, 3, 7, 7, 7, 3, 4, 3, 2, 3, 7, 4, 7, 4, 6, 2, 7, 2, 3, 7, 7, 4, 7, 1, 7, 2, 6, 7, 7, 7, 7, 3, 2, 2, 4, 7, 3, 6, 3, 5, 7, 0, 5, 3, 5, 7, 6, 7, 2, 0, 5, 5, 6, 1, 5, 0, 3, 1, 5, 2, 7, 2, 4, 3, 0, 5, 7, 2, 4, 1, 2, 3, 3, 0, 1, 0, 0, 0, 1, 3, 7, 1, 3, 7, 1, 4, 4, 3, 5, 6, 2, 7, 5, 1, 1, 0, 0, 7, 4, 4, 5, 4, 1, 1, 1, 4, 5, 0, 4, 2, 5, 2, 2, 3, 1, 4, 1, 1, 1, 4, 7, 1, 4, 5, 0, 7, 7, 1, 1, 0, 5, 7, 0, 4, 0, 4, 7, 6, 1, 5, 0, 0, 0, 4, 4, 2, 1, 2, 5, 4, 2, 2, 6, 4, 1, 2, 4, 6, 6, 2, 4, 5, 7, 5, 3, 3, 3, 2, 2, 5, 4, 7, 4, 4, 4, 4, 5, 3, 3, 5, 2, 5, 2, 7, 2, 5, 0, 3, 0, 0, 2, 4, 5, 3, 1, 4, 5, 6, 0, 0, 1, 7, 4, 7, 2, 1, 0, 3, 6, 3, 3, 3, 3, 3, 7, 7, 3, 4, 2, 1, 3, 3, 1, 3, 3, 6, 7, 1, 7, 1, 4, 4, 1, 3, 6, 4, 3, 1, 0, 6, 3, 0, 2, 3, 4, 3, 6, 3, 2, 1, 1, 5, 5, 4, 4, 4, 3, 2, 6, 6, 6, 1, 5, 3, 7, 6, 5, 7, 7, 3, 0, 0, 3, 6, 6, 6, 5, 0, 0, 1, 7, 0, 7, 0, 2, 7, 7, 0, 1, 4, 5, 7, 5, 3, 7, 6, 2, 2, 1, 4, 5, 7, 1, 1, 3, 4, 2, 7, 3, 7, 7, 7, 7, 7, 4, 4, 1, 4, 4, 2, 7, 7, 7, 7, 7, 4, 4, 7, 7, 3, 5, 3, 6, 7, 7, 4, 1, 3, 4, 7, 4, 2, 4, 7, 0, 2, 0, 3, 4, 2, 3, 4, 7, 7, 7, 1, 7, 6, 2, 1, 7, 4, 3, 1, 2, 4, 1, 3, 1, 1, 5, 3, 5, 5, 2, 5, 3, 0, 6, 4, 3, 7, 3, 1, 2, 5, 5, 5, 7, 2, 1, 3, 0, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2xEmsC0A2bp",
        "outputId": "4c23840f-5c43-48f3-9bca-5e27d177c5a5"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "# compute contingency matrix (also called confusion matrix)\n",
        "contingency_matrix = metrics.cluster.contingency_matrix(list_labels, y_pred)\n",
        "# return purity\n",
        "np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5170821791320406"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_SQrTBhDdfy",
        "outputId": "877eaa8d-d3d9-4cc0-d6fb-7f596ac58593"
      },
      "source": [
        "from sklearn import metrics\n",
        "ARI = metrics.adjusted_rand_score(list_labels, y_pred)\n",
        "print(ARI)\n",
        "AMI1 = metrics.adjusted_mutual_info_score(list_labels, y_pred)\n",
        "print('AMI= ',AMI1)\n",
        "# 基于标准化互信息指数的相似度亦为1\n",
        "NMI1 = metrics.normalized_mutual_info_score(list_labels, y_pred)\n",
        "print('NMI1= ',NMI1)\n",
        "# 基于互信息指数的相似度不为1，不符合理论\n",
        "MI = metrics.mutual_info_score(list_labels, y_pred)\n",
        "print('MI= ',MI)\n",
        "homogeneity=metrics.homogeneity_score(list_labels, y_pred)\n",
        "print('homogeneity= ',homogeneity)\n",
        "completeness=metrics.completeness_score(list_labels, y_pred)\n",
        "print('completeness= ',completeness)\n",
        "completeness=metrics.v_measure_score(list_labels, y_pred)\n",
        "print('completeness= ',completeness)\n",
        "score = metrics.adjusted_rand_score(list_labels, y_pred)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.003583719340914973\n",
            "AMI=  0.039013325958078666\n",
            "NMI1=  0.052469279715663904\n",
            "MI=  0.08984534416808754\n",
            "homogeneity=  0.057443126300414045\n",
            "completeness=  0.04828813898334739\n",
            "completeness=  0.0524692797156639\n",
            "0.003583719340914973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "qAuBYY60GhBP",
        "outputId": "38cfc9d5-30c6-4def-d573-506d036d8552"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE()\n",
        "tsne.fit_transform(corpus_embeddings) #进行数据降维\n",
        "tsne = pd.DataFrame(tsne.embedding_) #转换数据格式\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei'] #用来正常显示中文标签\n",
        "plt.rcParams['axes.unicode_minus'] = False #用来正常显示负号\n",
        "#不同类别用不同颜色和样式绘图\n",
        "d = tsne[['cluster'] == 0]\n",
        "plt.plot(d[0], d[1], 'r.')\n",
        "d = tsne[['cluster'] == 1]\n",
        "plt.plot(d[0], d[1], 'go')\n",
        "d = tsne[['cluster'] == 2]\n",
        "plt.plot(d[0], d[1], 'b*')\n",
        "d = tsne[['cluster'] == 3]\n",
        "plt.plot(d[0], d[1], 'y.')\n",
        "d = tsne[['cluster'] == 4]\n",
        "plt.plot(d[0], d[1], 'g+')\n",
        "d = tsne[['cluster'] == 5]\n",
        "plt.plot(d[0], d[1], 'bo')\n",
        "d = tsne[['cluster'] == 6]\n",
        "plt.plot(d[0], d[1], 'y*')\n",
        "d = tsne[['cluster'] == 7]\n",
        "plt.plot(d[0], d[1], 'r*')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-184-3d9b422879da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'axes.unicode_minus'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m#用来正常显示负号\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#不同类别用不同颜色和样式绘图\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2876\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2877\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2878\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2880\u001b[0m         \u001b[0;31m# Do we have a slicer (on rows)?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3540\u001b[0m             \u001b[0;31m#  pending resolution of GH#33047\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3542\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3543\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3544\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_col_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: False"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHQBTed_KH3F",
        "outputId": "97136db9-8338-4906-ee28-f8f6f3f72e86"
      },
      "source": [
        "from sklearn import metrics\n",
        "labels_true = [0, 0, 0, 1, 1, 1]\n",
        "labels_pred = [0, 0, 1, 1, 2, 2]\n",
        "\n",
        "# 基本用法\n",
        "score = metrics.adjusted_rand_score(labels_true, labels_pred)\n",
        "print(score)\n",
        "\n",
        "# 与标签名无关\n",
        "labels_pred = [5, 5, 0, 0, 7, 7]\n",
        "score = metrics.adjusted_rand_score(labels_true, labels_pred)\n",
        "print(score)\n",
        "\n",
        "# 具有对称性\n",
        "score = metrics.adjusted_rand_score(labels_pred, labels_true)\n",
        "print(score)\n",
        "\n",
        "# 接近 1 最好\n",
        "labels_pred = labels_true[:]\n",
        "score = metrics.adjusted_rand_score(labels_true, labels_pred)\n",
        "print(score)\n",
        "\n",
        "# 独立标签结果为负或者接近 0\n",
        "labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n",
        "labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n",
        "score = metrics.adjusted_rand_score(labels_true, labels_pred)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.24242424242424246\n",
            "0.24242424242424246\n",
            "0.24242424242424246\n",
            "1.0\n",
            "-0.12903225806451613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rQhz_ELPq2_",
        "outputId": "ddd8538f-7b8a-4d9e-b21a-74638d0fab02"
      },
      "source": [
        "with open(\"/content/bert_finetune390_cls_kmeans_test.txt\", 'r') as f:\n",
        "  for line in f:\n",
        "    line = line.replace(\",\",\"\")\n",
        "    line = line.replace(\"[\",\"\")\n",
        "    labels1 = line.replace(\"]\",\"\")\n",
        "#print(labels1)\n",
        "#print(true_labels)\n",
        "#labels1 = kn_pre[:1083]\n",
        "c1 = 0\n",
        "c2 = 0\n",
        "c3 = 0\n",
        "c4 = 0\n",
        "c5 = 0\n",
        "c6 = 0\n",
        "c7 = 0\n",
        "c8 = 0\n",
        "for i in range(391,len(labels1)):\n",
        "  if(labels1[i]=='0' and true_labels[i]=='1'):\n",
        "    c1 += 1\n",
        "  if(labels1[i]=='0' and true_labels[i]=='2'):\n",
        "    c2 += 1\n",
        "  if(labels1[i]=='0' and true_labels[i]=='3'):\n",
        "    c3 += 1\n",
        "  if(labels1[i]=='0' and true_labels[i]=='4'):\n",
        "    c4 += 1\n",
        "  if(labels1[i]=='0' and true_labels[i]=='5'):\n",
        "    c5 += 1\n",
        "  if(labels1[i]=='0' and true_labels[i]=='6'):\n",
        "    c6 += 1\n",
        "  if(labels1[i]=='0' and true_labels[i]=='7'):\n",
        "    c7 += 1\n",
        "  if(labels1[i]=='0' and true_labels[i]=='8'):\n",
        "    c8 += 1\n",
        "print(c1,c2,c3,c4,c5,c6,c7,c8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9 5 5 3 7 60 20 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlM_XtiGXSyN"
      },
      "source": [
        "M_aggregate = [[159,0,4,0,28,0,0,0],\n",
        "         [2,57,122,0,0,0,0,0],\n",
        "         [0,0,0,0,0,36,66,0],\n",
        "         [0,0,0,0,0,0,17,0],\n",
        "         [29,0,0,0,155,0,0,0],\n",
        "         [0,122,4,0,0,39,3,0],\n",
        "         [61,0,116,0,0,0,0,0],\n",
        "         [0,0,0,19,0,0,0,0]]\n",
        "m_i = [191,181,102,17,184,168,177,19]\n",
        "m = 1039"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoHAjUzop8_g"
      },
      "source": [
        "M_aggregate = [[3,4,9,0,19,104,32,9],\n",
        "         [2,12,17,6,24,106,27,13],\n",
        "         [0,13,30,3,25,113,38,15],\n",
        "         [3,3,4,4,5,45,10,3],\n",
        "         [8,3,8,3,5,60,9,5],\n",
        "         [8,2,3,0,0,18,2,3],\n",
        "         [11,0,0,0,1,8,1,0],\n",
        "         [0,20 ,35 ,7 ,12, 102, 36, 12]]\n",
        "m_i = [180,207,237,77,101,36,21,224]\n",
        "m = 1039"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vd-NDbkrikS"
      },
      "source": [
        "M_aggregate = [[9,5,5,3,7,60,20,6],\n",
        "[0,7,14,6,23,89,22,14],\n",
        "[0,11,23,3,10,66,23,7],\n",
        "[10,0,0,0,1,6,1,0],\n",
        "[6,2,1,1,0,11,1,3],\n",
        "[1,1,4,3,5,51,10,3],\n",
        "[0,12,17,5,16,76,36,11],\n",
        "[3,2,13,0,25,80,26,11]]\n",
        "m_i = [155,175,143,18,25,78,173,160]\n",
        "m = 887"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aSkshNFSjTy"
      },
      "source": [
        "M_aggregate = [[49,25,51,0,38,9,13,0],\n",
        "         [51,69,62,0,38,16,9,0],\n",
        "         [15,16,17,1,8,9,7,0],\n",
        "         [15,6,8,0,13,4,2,0],\n",
        "         [52,39,67,0,24,9,5,0],\n",
        "         [69,18,41,0,62,6,3,0],\n",
        "         [0,2,0,12,0,8,3,0],\n",
        "         [0,4,0,6,0,14,44,0]]\n",
        "m_i = [185,245,73,48,196,195,25,68]\n",
        "m = 1083"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2YpFs6gYIJw",
        "outputId": "8a195334-e345-4b50-baa7-e7aeeed005f8"
      },
      "source": [
        "import math\n",
        "e_i = [0 for x in range(0,8)]\n",
        "p_i = [0 for x in range(0,8)]\n",
        "for i in range(0,8):\n",
        "\twr_line_part = \",\"\n",
        "\tfor j in range(0,8):\n",
        "\t\twr_line_part += str(M_aggregate[i][j]) + ','\n",
        "\t\tp_i_j = M_aggregate[i][j]*1.0/m_i[i]  + 0.00000001\n",
        "\t\t#print (p_i_j)\n",
        "\t\te_i[i] += 0 - p_i_j*math.log2(p_i_j)\n",
        "\t\tif (p_i[i] < p_i_j):\n",
        "\t\t\tp_i[i] = p_i_j\n",
        "\t#print (e_i[i])\n",
        "\tprint (p_i[i])\n",
        "\n",
        "e = 0\n",
        "p = 0\n",
        "for i in range(0,8):\n",
        "\te += m_i[i]/m*e_i[i]\n",
        "\tp += m_i[i]/m*p_i[i]\n",
        "\n",
        "#print(e)\n",
        "print(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.38709678419354837\n",
            "0.5085714385714286\n",
            "0.46153847153846156\n",
            "0.5555555655555556\n",
            "0.44000001\n",
            "0.6538461638461539\n",
            "0.4393063683815029\n",
            "0.50000001\n",
            "0.4994363125930101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfDR_n-dBqUI"
      },
      "source": [
        "Plan3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJVK97bgBrQL"
      },
      "source": [
        "! python -m pip install --upgrade pip --quiet\n",
        "! pip install spacy --quiet\n",
        "! pip3 install spacy-transformers --quiet\n",
        "! python -m spacy download en_trf_bertbaseuncased_lg --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZP3p_kgGDGV"
      },
      "source": [
        "! python -m spacy download en_trf_bertbaseuncased_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgE3TPiVGwIK"
      },
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3Evv6qyGeAO"
      },
      "source": [
        "!python -m spacy download en_trf_bertbaseuncased_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_5-CGgiBs_Q"
      },
      "source": [
        "#Load bert model\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#nlp = en_trf_bertbaseuncased_lg.load()\n",
        "\n",
        "# Utility function for generating sentence embedding from the text\n",
        "def get_embeddinngs(text):\n",
        "    return nlp(text).vector\n",
        "\n",
        "# Generating sentence embedding from the text\n",
        "data_embedding = corpus.apply(get_embeddinngs)\n",
        "print(data_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn91sFfaErKR",
        "outputId": "4a73d0c9-5769-467d-e8ee-834df285ac9e"
      },
      "source": [
        "!python -m en_pytt_bertbaseuncased_lg info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/bin/python3: No module named en_pytt_bertbaseuncased_lg.__main__; 'en_pytt_bertbaseuncased_lg' is a package and cannot be directly executed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVsmYl4LKak2"
      },
      "source": [
        "Plan4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3pBK9xsKcOG"
      },
      "source": [
        "!pip install bert-serving-server  # server\n",
        "!pip install bert-serving-client\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqmGczzHKqbf"
      },
      "source": [
        "!bert-serving-start -model_dir /content/bert/ -num_worker=4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS3ZLJFIL2yE"
      },
      "source": [
        "!conda create -n test python=3.7.6\n",
        "!conda activate test\n",
        "!pip install tensorflow==1.15.0\n",
        "!pip install bert-serving-server"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhLX6kN8MBl6"
      },
      "source": [
        "!bert-serving-start -model_dir /content/bert/ -num_worker=4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs8IcAbbnwd-"
      },
      "source": [
        "Plan5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0A2PACDnwNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cecb1eb-eb63-4953-eacb-8cd64a786459"
      },
      "source": [
        "!pip install bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/e6/55ed98ef52b168a38192da1aff7265c640f214009790220664ee3b4cb52a/bert-2.2.0.tar.gz\n",
            "Collecting erlastic\n",
            "  Downloading https://files.pythonhosted.org/packages/f3/30/f40d99fe35c38c2e0415b1e746c89569f2483e64ef65d054b9f0f382f234/erlastic-2.0.0.tar.gz\n",
            "Building wheels for collected packages: bert, erlastic\n",
            "  Building wheel for bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert: filename=bert-2.2.0-cp37-none-any.whl size=3755 sha256=c8e7043d89a65695df1327aeb804dc9c235b19d568ccccce7540fe7e72b2df4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/71/b7/941459453bd38e5d97a8c886361dee19325e9933c9cf88ad46\n",
            "  Building wheel for erlastic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for erlastic: filename=erlastic-2.0.0-cp37-none-any.whl size=6787 sha256=69137741cb4a83ab79f8ab90ca0f219cc90a2db9fecd49f6498a4734f7153b50\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/62/46/93c713a5f061aeeb4f16eb6bf5ee798816e6ddda70faa78e69\n",
            "Successfully built bert erlastic\n",
            "Installing collected packages: erlastic, bert\n",
            "Successfully installed bert-2.2.0 erlastic-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb9w-MVtn8Qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c1217b-7d64-4efa-94ff-b1d5979d8c2e"
      },
      "source": [
        "!git clone https://github.com/google-research/bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert'...\n",
            "remote: Enumerating objects: 340, done.\u001b[K\n",
            "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
            "Receiving objects: 100% (340/340), 328.28 KiB | 1.74 MiB/s, done.\n",
            "Resolving deltas: 100% (182/182), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtiEm8EZoPfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3344614a-f90d-4616-a2a8-6b31a75005dc"
      },
      "source": [
        "cd /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAVBfzNmoehV"
      },
      "source": [
        "!echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /content/input.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "mdRiOehoNhzw",
        "outputId": "61dd5b10-504e-492f-b755-506bfc2771e0"
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/29/6b4f1e02417c3a1ccc85380f093556ffd0b35dc354078074c5195c8447f2/tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 75kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.5MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (56.0.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Installing collected packages: tensorboard, keras-applications, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESBu-TmgoDvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725bbf79-3c30-4b5d-9fa9-621119a09f9c"
      },
      "source": [
        "!python /content/bert/extract_features.py \\\n",
        "  --input_file=/content/input.txt \\\n",
        "  --output_file=/content/output.json \\\n",
        "  --vocab_file=/content/vocab.txt \\\n",
        "  --bert_config_file=/content/bert_config.json \\\n",
        "  --init_checkpoint=/content/saved_weights.pt \\\n",
        "  --layers=-1,-2,-3,-4 \\\n",
        "  --max_seq_length=128 \\\n",
        "  --batch_size=8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:unique_id: 0\n",
            "INFO:tensorflow:tokens: [CLS] who was jim henson ? [SEP] jim henson was a puppet ##eer [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2040 2001 3958 27227 1029 102 3958 27227 2001 1037 13997 11510 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f49ec82cb00>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpzjn8oxkh\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpzjn8oxkh', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f49e0517cd0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': None}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
            "INFO:tensorflow:Could not find trained model in model_dir: /tmp/tmpzjn8oxkh, running initialization to predict.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Running infer on CPU\n",
            "WARNING:tensorflow:From /content/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "2021-04-27 02:52:42.429337: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /content/saved_weights.pt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n",
            "INFO:tensorflow:Error recorded from prediction_loop: Unable to open table file /content/saved_weights.pt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n",
            "INFO:tensorflow:prediction_loop marked as finished\n",
            "WARNING:tensorflow:Reraising captured error\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/bert/extract_features.py\", line 419, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"/content/bert/extract_features.py\", line 389, in main\n",
            "    for result in estimator.predict(input_fn, yield_single_examples=True):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2500, in predict\n",
            "    rendezvous.raise_errors()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py\", line 128, in raise_errors\n",
            "    six.reraise(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/six.py\", line 703, in reraise\n",
            "    raise value\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2494, in predict\n",
            "    yield_single_examples=yield_single_examples):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 611, in predict\n",
            "    features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2251, in _call_model_fn\n",
            "    config)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1112, in _call_model_fn\n",
            "    model_fn_results = self._model_fn(features=features, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2534, in _model_fn\n",
            "    features, labels, is_export_mode=is_export_mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1323, in call_without_tpu\n",
            "    return self._call_model_fn(features, labels, is_export_mode=is_export_mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1593, in _call_model_fn\n",
            "    estimator_spec = self._model_fn(features=features, **kwargs)\n",
            "  File \"/content/bert/extract_features.py\", line 175, in model_fn\n",
            "    tvars, init_checkpoint)\n",
            "  File \"/content/bert/modeling.py\", line 330, in get_assignment_map_from_checkpoint\n",
            "    init_vars = tf.train.list_variables(init_checkpoint)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/checkpoint_utils.py\", line 95, in list_variables\n",
            "    reader = load_checkpoint(ckpt_dir_or_file)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/checkpoint_utils.py\", line 64, in load_checkpoint\n",
            "    return pywrap_tensorflow.NewCheckpointReader(filename)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 326, in NewCheckpointReader\n",
            "    return CheckpointReader(compat.as_bytes(filepattern), status)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
            "    c_api.TF_GetCode(self.status.status))\n",
            "tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /content/saved_weights.pt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMli0-F9XL7x"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import os\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "nonstop_text = []\n",
        "f = open(\"/content/longreviews_sentences.txt\", \"r\")\n",
        "for paragraph in f:\n",
        "\tnonstop_text.append(paragraph)\n",
        "\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(nonstop_text)\n",
        "text = vectorizer.transform(nonstop_text)\n",
        "\n",
        "#find_k(text, 40)\n",
        "\n",
        "clusters = KMeans(n_clusters = 8).fit_predict(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c35jWYG-dq68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ba7e84-9ad0-4c89-b8f0-541587b9ba05"
      },
      "source": [
        "from sklearn.cluster import Birch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import os\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(nonstop_text)\n",
        "text = vectorizer.transform(nonstop_text)\n",
        "\n",
        "ss = Birch(branching_factor=10, n_clusters = 8, threshold=0.5,compute_labels=True)\n",
        "birch_pre = ss.fit_predict(text)\n",
        "print(birch_pre)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 3 0 ... 7 3 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DenoZkMqd4Vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a092a97-f0bf-45e8-f027-2198108d0079"
      },
      "source": [
        "import hdbscan\n",
        "from sklearn.cluster import Birch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import os\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(nonstop_text)\n",
        "text = vectorizer.transform(nonstop_text)\n",
        "\n",
        "cluster = hdbscan.HDBSCAN(min_cluster_size=8,metric='euclidean',cluster_selection_method='eom').fit(text)\n",
        "cluster_assignment = cluster.labels_\n",
        "print(cluster_assignment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkauWWPxedRx",
        "outputId": "fbefdb54-d882-4e4c-e391-278ba10424b5"
      },
      "source": [
        "import sklearn\n",
        "from sklearn import metrics\n",
        "print(metrics.calinski_harabaz_score(text.toarray(),cluster_assignment))\n",
        "print(metrics.silhouette_score(text.toarray(),cluster_assignment, metric='euclidean'))\n",
        "#sklearn.metrics.davies_bouldin_score(text.toarray(),cluster_assignment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function calinski_harabaz_score is deprecated; Function 'calinski_harabaz_score' has been renamed to 'calinski_harabasz_score' and will be removed in version 0.23.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.8983622656031272\n",
            "0.010699355252069958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7B9hznlhO1o"
      },
      "source": [
        "y_pred = clusters[:1083]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdIXISwMgawW",
        "outputId": "6120a68a-c611-4a60-e6a8-426b5d1d3235"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "# compute contingency matrix (also called confusion matrix)\n",
        "contingency_matrix = metrics.cluster.contingency_matrix(list_labels, y_pred)\n",
        "# return purity\n",
        "np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5133887349953832"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYCoL0cCgfkL",
        "outputId": "51c60427-83c0-4532-aea3-e090fe35bcce"
      },
      "source": [
        "from sklearn import metrics\n",
        "ARI = metrics.adjusted_rand_score(list_labels, y_pred)\n",
        "print(ARI)\n",
        "AMI1 = metrics.adjusted_mutual_info_score(list_labels, y_pred)\n",
        "print('AMI= ',AMI1)\n",
        "# 基于标准化互信息指数的相似度亦为1\n",
        "NMI1 = metrics.normalized_mutual_info_score(list_labels, y_pred)\n",
        "print('NMI1= ',NMI1)\n",
        "# 基于互信息指数的相似度不为1，不符合理论\n",
        "MI = metrics.mutual_info_score(list_labels, y_pred)\n",
        "print('MI= ',MI)\n",
        "homogeneity=metrics.homogeneity_score(list_labels, y_pred)\n",
        "print('homogeneity= ',homogeneity)\n",
        "completeness=metrics.completeness_score(list_labels, y_pred)\n",
        "print('completeness= ',completeness)\n",
        "completeness=metrics.v_measure_score(list_labels, y_pred)\n",
        "print('completeness= ',completeness)\n",
        "score = metrics.adjusted_rand_score(list_labels, y_pred)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0040488969112450245\n",
            "AMI=  0.03811098652640065\n",
            "NMI1=  0.05141087191665908\n",
            "MI=  0.08899456315271898\n",
            "homogeneity=  0.0568991746713972\n",
            "completeness=  0.04688819458453334\n",
            "completeness=  0.05141087191665908\n",
            "0.0040488969112450245\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}